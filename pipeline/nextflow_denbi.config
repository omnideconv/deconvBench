params {

  /*** input directories and files ***/
  
  data_dir_bulk = "/vol/omnideconv_input/omnideconv_data/PBMC/"                                 // directory to bulk RNA-seq datasets
  data_dir_sc = "/vol/omnideconv_input/omnideconv_data/singleCell/"                             // directory to scRNA-seq datasets
  
  /*** output directories and files ***/
  
  results_dir_general = "/vol/omnideconv_results/results_downsample"                            // here all final results are stored
  preProcess_dir = "/vol/omnideconv_input/preprocess"                                           // directory where intermediate files for preprocessing or subsampling are stored
  workDir = "/vol/omnideconv_input/work"                                                        // working directory for nextflow

  
  /*** parameters for benchmarking ***/
  
  //single_cell_list = ["hao-sampled-1","hao-sampled-2","hao-sampled-3","hao-sampled-4"]        // list of scRNA-seq dataset names that are used (no underscores in the names allowed!)
  //single_cell_norm = ["counts","cpm"]                                                         // list of normalization methods for scRNA-seq datasets
  //bulk_list = ["finotello","finotello-simulation","hoek","hoek-simulation"]                   // list of bulk RNA-seq dataset names that are used
  //bulk_norm = ["counts","tpm"]                                                                // list of normalization methods for bulk RNA-seq datasets
  
  //method_list = ["momf","music","scdc","scaden","bisque","cibersortx","bayesprism","autogenes","dwls"]  // list of deconvolution methods
  
  ct_fractions = [5,10]                                                                          // cell-type fractions for subsampling    
  replicates = 5                                                                                // number of replicates for subsampling
  
  /*** parameters for simulations ***/
  
  simulation_sc_dataset = "hao-complete"                                                        // name of sc dataset that will be used as basis for simulations
  simulation_sc_norm = "counts"                                                                 // normalization of sc dataset that will be used as basis for simulations
  simulation_n_cells = [1000, 2000]                                                             // number of cells in a pseudo-bulk sample
  simulation_scenario = ['random']                                                              // simulation scenario
  simulation_n_samples = [10]                                                                   // number of pseudo-bulk samples to be generated per simulation run
  
  /*** other parameters ***/
  
  publishDirMode = "copy"
  ncores = '10'


  single_cell_list = ["hao-complete"]
  single_cell_norm = ["counts","cpm"]
  bulk_list = ["finotello"]
  bulk_norm = ["tpm","counts"]  
  method_list = ["scaden"] // "bisque","music","cibersortx","bayesprism","autogenes","dwls","scdc","scaden"

  
}


profiles {
     standard {
        process.executor = 'local'
        process.max_retries = 50
        process.cpus = 10
        process.memory = '50 GB'
        docker.enabled = true
        process.container = 'omnideconv_benchmark:latest'
        docker.temp = "auto"
     }
     cluster {
        process.executor = 'slurm'
        process.cpus = 10    // this parameter is ignored in slurm if docker is used
        process.memory = '100 GB'
        docker.enabled = true
        process.container = 'omnideconv_benchmark:latest'
        docker.temp = "auto"
        docker.cacheDir = "/vol/spool/tmp"
     }
     cluster_dwls {
        process.executor = 'slurm'
        process.cpu = '24'
        process.memory = '200 GB'
        docker.enabled = true
        process.container = 'omnideconv_benchmark:latest'
        containerOptions = "--user ubuntu"
        docker.temp = "auto"
     }
}

docker {  
  runOptions = '--cpus 10 -v /var/run/docker.sock:/var/run/docker.sock -v /vol/omnideconv_results:/vol/omnideconv_results'
}

dag {
  overwrite = true
}

report {
  overwrite = true
}

trace {
  enabled = false
  file = '/vol/omnideconv_results/trace_<timestamp>.txt'
  fields = 'task_id,process,name,status,module,container,cpus,time,disk,memory,realtime,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,script'
  sep = ','
  overwrite = true
}



// start on cluster like this:
// nextflow -C /vol/omnideconv_input/benchmark/pipeline/nextflow_denbi.config run /vol/omnideconv_input/benchmark/pipeline/runOmnideconvBenchmark.nf -profile cluster -w /vol/omnideconv_input/work/

// stop all slurm jobs:
// squeue -u "ubuntu"| awk '{print $1}' | tail -n+2 | xargs scancel

// start container manually:
// docker run -it --memory 122880m -v /vol/omnideconv_input:/vol/omnideconv_input -v /vol/omnideconv_results:/vol/omnideconv_results -v /var/run/docker.sock:/var/run/docker.sock --cpus 14 omnideconv_benchmark:latest
// in R:
// library(omnideconv)
// sc_matrix <- readRDS('/vol/omnideconv/omnideconv_data/singleCell/hao-sampled-1/matrix_norm_counts.rds')
// sc_celltype_annotations <- readRDS('/vol/omnideconv/omnideconv_data/singleCell/hao-sampled-1/celltype_annotations.rds')
// sc_batch <- readRDS('/vol/omnideconv/omnideconv_data/singleCell/hao-sampled-1/batch.rds')
// bulk_matrix <- readRDS('vol/omnideconv/omnideconv_data/PBMC/hoek/hoek_tpm.rds')
// sig <- readRDS('/vol/omnideconv/results_run/music_hao-sampled-1_cpm_hoek_tpm_ct0.02_rep2/signature.rds')
